<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yan Wang - Research Scientist and Tech Lead at NVIDIA Research</title>
  <meta name="author" content="Yan Wang">
  <meta name="description" content="Yan Wang is a Research Scientist and Tech Lead at NVIDIA Research specializing in autonomous driving, VLMs, VLA models, and reasoning. Ph.D. from Cornell University.">
  <meta name="keywords" content="Yan Wang, NVIDIA Research, Autonomous Driving, VLA Models, Vision Language Action, Alpamayo-R1, Chain of Causation, Pseudo-LiDAR, STORM, Cornell University, Waymo, Computer Vision, Machine Learning, 3D Object Detection, End-to-End Driving, Multimodal Foundation Models, Reasoning AI, Trajectory Planning, Self-Driving Cars">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="Yan Wang - Research Scientist and Tech Lead at NVIDIA Research">
  <meta property="og:description" content="Research Scientist and Tech Lead at NVIDIA Research specializing in autonomous driving, VLMs, VLA models, and reasoning. Ph.D. from Cornell University.">
  <meta property="og:image" content="https://yanwang.org/images/yan_wang.JPG">
  <meta property="og:url" content="https://yanwang.org/">
  <meta name="twitter:card" content="summary">
  <link rel="canonical" href="https://yanwang.org/">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-N79WVZLZB4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-N79WVZLZB4');
  </script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <!-- Introduction Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Yan Wang
                  </p>
                  <p>
                    I am a Research Scientist and Tech Lead at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, specializing in autonomous driving, VLMs, VLA models, and reasoning.
                  <!-- </p> -->
                  <!-- <p> -->
                    I received my Ph.D. in Computer Science from <a href="https://www.cs.cornell.edu/">Cornell University</a>, where I was advised by <a href="https://www.cs.cornell.edu/~kilian/">Kilian Q. Weinberger</a> and <a href="https://www.cs.cornell.edu/~bharathh/">Bharath Hariharan</a>. Previously, I was a Research Scientist at <a href="https://waymo.com/">Waymo</a>. 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:yanwang1993@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://github.com/mileyan">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/yanwang8">Linkedin</a> &nbsp;/&nbsp;
                    <a href="https://x.com/yan_wang_9">X</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=nZsD8XwAAAAJ">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/yan_wang.JPG">
                    <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;" alt="profile photo" src="images/yan_wang.JPG">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm interested in Multimodal foundation models, 3D computer vision, autonomous driving. 
                    My research focuses on developing end-to-end learning systems for autonomous vehicles, 
                    and let the driving model can think itself.
                    I'm particularly interested in vision-language-action (VLA) models with reasoning capabilities 
                    and world models for embodied AI.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- News Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>News</h2>
                  <div style="max-height:300px;overflow-y:auto;padding-right:10px;">
                    <ul>
                      <li style="margin-bottom:10px;">
                        <strong>[Dec 2025]</strong> Keynote talk at <a href="https://mozhgan91.github.io/vlm4rwd-neurips25-ws/">NeurIPS 2025 Workshop on VLM4RWD</a> on reasoning VLA for autonomous vehicles.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Oct 2025]</strong> Released <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/Alpamayo-R1_1.pdf">Alpamayo-R1</a>, a reasoning VLA model for autonomous driving.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Oct 2025]</strong> Keynote at <a href="https://drivex-workshop.github.io/iccv2025/">ICCV 2025 DriveX Workshop</a> on reasoning models for physical AI.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Oct 2025]</strong> 1 paper presented at <a href="https://iccv2025.thecvf.com/">ICCV 2025</a>.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Jul 2025]</strong> 1 paper accepted by <a href="https://colmweb.org/">COLM 2025</a>.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Feb 2025]</strong> 1 paper accepted by <a href="https://2025.ieee-icra.org/">ICRA 2025</a>.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Jan 2025]</strong> 2 papers accepted by <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a>.
                      </li>
                      <li style="margin-bottom:10px;">
                        <strong>[Sep 2024]</strong> 2 papers accepted by <a href="https://neurips.cc/Conferences/2024">NeurIPS 2024</a>.
                      </li>
                    </ul>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Selected Publications</h2>
                  <p>
                    For a complete list of publications, please visit my <a href="https://scholar.google.com/citations?user=nZsD8XwAAAAJ">Google Scholar</a> profile.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- Alpamayo-R1 Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/alpamayo_r1.png" alt="Alpamayo-R1" style="width:160px;height:160px;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/Alpamayo-R1_1.pdf">
                    <span class="papertitle">Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</span>
                  </a>
                  <br>
                  <span style="display:inline-block;margin-top:6px;margin-bottom:6px;">Project Lead</span>
                  <br>
                  <em>NVIDIA Technical Report</em>, 2025
                  <br>
                  <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/Alpamayo-R1_1.pdf">PDF</a>
                  <br>
                  <p></p>
                  <p>
                    A vision-language-action model integrating Chain of Causation reasoning with trajectory planning, demonstrating improvements on open-loop, closed-loop evaluation, and real-world road tests.
                  </p>
                </td>
              </tr>

              <!-- Paper 1 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cube_llm.png" alt="CubeLLM" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/pdf?id=yaQbTAD2JJ">
                    <span class="papertitle">Language-Image Models with 3D Understanding</span>
                  </a>
                  <br>
                  <a href="https://janghyuncho.github.io/">Jang Hyun Cho</a>, <a href="https://www.borisivanovic.com/">Boris Ivanovic</a>, <a href="https://kikacaty.github.io/">Yulong Cao</a>, <a href="https://research.nvidia.com/person/ed-schmerling">Edward Schmerling</a>, <a href="https://yuewang.xyz/">Yue Wang</a>, <a href="https://research.nvidia.com/person/xinshuo-weng">Xinshuo Weng</a>, <a href="https://sites.google.com/site/boyilics/home">Boyi Li</a>, <a href="https://yurongyou.com/">Yurong You</a>, <a href="https://www.philkr.net/">Philipp Kr√§henb√ºhl</a>*, <strong>Yan Wang*</strong>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>*
                  <br>
                  <span style="font-size:12px;">* Co-advised</span>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://openreview.net/pdf?id=yaQbTAD2JJ">OpenReview</a>
                  <br>
                  <p></p>
                  <p>
                    Bridging vision-language models (VLMs) with 3D understanding.
                  </p>
                </td>
              </tr>

              <!-- STORM Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/storm.png" alt="STORM" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/pdf?id=M2NFWRPMUd">
                    <span class="papertitle">STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes</span>
                  </a>
                  <br>
                  <a href="https://jiawei-yang.github.io/">Jiawei Yang</a>, <a href="https://huangjh-pub.github.io/">Jiahui Huang</a>, <a href="https://research.nvidia.com/person/yuxiao-chen">Yuxiao Chen</a>, <strong>Yan Wang</strong>, <a href="https://sites.google.com/site/boyilics/home">Boyi Li</a>, <a href="https://yurongyou.com/">Yurong You</a>, <a href="https://maximilianigl.com/">Maximilian Igl</a>, <a href="https://scholar.google.com/citations?user=uxiJcasAAAAJ&hl=en">Apoorva Sharma</a>, <a href="https://www.peterkarkus.com/">Peter Karkus</a>, <a href="https://faculty.cc.gatech.edu/~danfei/">Danfei Xu</a>, <a href="https://www.borisivanovic.com/">Boris Ivanovic</a>, <a href="https://yuewang.xyz/">Yue Wang</a>, <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://openreview.net/pdf?id=M2NFWRPMUd">OpenReview</a>
                  <br>
                  <p></p>
                  <p>
                    A feed-forward, self-supervised method for fast and accurate reconstruction of dynamic 3D scenes from sparse, multi-timestep, posed camera images.
                  </p>
                </td>
              </tr>

              <!-- Test-Time Scaling Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/test_time.png" alt="Test-Time Scaling" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2503.24320">
                    <span class="papertitle">Can Test-Time Scaling Improve World Foundation Model?</span>
                  </a>
                  <br>
                  <a href="https://www.wenyancong.com/">Wenyan Cong</a>, <a href="https://zhuhanqing.github.io/">Hanqing Zhu</a>, <a href="https://peihaowang.github.io/">Peihao Wang</a>, <a href="https://scholar.google.com/citations?user=yWuQY0UAAAAJ">Bangya Liu</a>, <a href="https://ir1d.github.io/">Dejia Xu</a>, <a href="https://scholar.google.com/citations?user=pncffqwAAAAJ&hl=en">Kevin Wang</a>, <a href="https://users.ece.utexas.edu/~dpan/">David Z. Pan</a>, <strong>Yan Wang</strong>, <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>, <a href="https://scholar.google.com/citations?user=pxFyKAIAAAAJ&hl=en">Zhangyang Wang</a>
                  <br>
                  <em>COLM</em>, 2025
                  <br>
                  <a href="https://arxiv.org/pdf/2503.24320">arXiv</a>
                  <br>
                  <p></p>
                  <p>
                    Exploring test-time scaling strategies for world foundation models.
                  </p>
                </td>
              </tr>

              <!-- Paper 2 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/pseudo_lidar.png" alt="Pseudo-LiDAR" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1812.07179">
                    <span class="papertitle">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving</span>
                  </a>
                  <br>
                  <strong>Yan Wang</strong>, <a href="https://sites.google.com/view/wei-lun-harry-chao/home">Wei-Lun Chao</a>, <a href="https://divyanshgarg.com/">Div Garg</a>, <a href="https://www.cs.cornell.edu/~bharathh/">Bharath Hariharan</a>, <a href="https://mae.cornell.edu/faculty-directory/mark-campbell">Mark Campbell</a>, <a href="https://www.cs.cornell.edu/~kilian/">Kilian Q. Weinberger</a>
                  <br>
                  <em>CVPR</em>, 2019
                  <br>
                  <a href="https://arxiv.org/abs/1812.07179">arXiv</a>
                  <br>
                  <p></p>
                  <p>
                    Proposing the Pseudo-LiDAR representation that bridges the gap between image-based and LiDAR-based 3D object detection.
                  </p>
                </td>
              </tr>

              <!-- Paper 5 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/waymax.png" alt="Waymax" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2310.08710">
                    <span class="papertitle">Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=lJbBWhMAAAAJ&hl=en">Cole Gulino</a>, <a href="https://scholar.google.com/citations?user=T9To2C0AAAAJ&hl=en">Justin Fu</a>, <a href="https://scholar.google.com/citations?user=os-DVLkAAAAJ&hl=en">Wenjie Luo</a>, <a href="https://scholar.google.com/citations?user=-gJkPHIAAAAJ&hl=en">George Tucker</a>, <a href="https://elibronstein.com/">Eli Bronstein</a>, <a href="https://luyiren.me/">Yiren Lu</a>, <a href="https://scholar.google.com/citations?user=5Qp_0TUAAAAJ&hl=en">Jean Harb</a>, <a href="https://scholar.google.com/citations?user=tlhfhLoAAAAJ&hl=en">Xinlei Pan</a>, <strong>Yan Wang</strong>, et al.
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2310.08710">arXiv</a>
                  /
                  <a href="https://github.com/waymo-research/waymax">code</a>
                  <br>
                  <p></p>
                  <p>
                    A high-performance, JAX-based simulator for autonomous driving research enabling large-scale RL training.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          <!-- Service Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Academic Service</h2>
                  <p>
                    <strong>Workshop Organizer:</strong><br>
                    - ICCV 2025: <a href="https://e2e3d.github.io/iccv2025/">End-to-End 3D Learning</a><br>
                    - ECCV 2024: <a href="https://mllmav.github.io/">Autonomous Vehicles meet Multimodal Foundation Models</a><br>
                    - CVPR 2023: <a href="https://cvpr2023.wad.vision">Autonomous Driving Workshop</a><br>
                    - CVPR 2022: <a href="https://cvpr2022.wad.vision">Autonomous Driving Workshop</a><br>
                    - CVPR 2021: <a href="https://www.adp3.org">Autonomous Driving Workshop</a>
                  </p>
                  <p>
                    <strong>Conference Reviewer:</strong> CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, ICRA, AAAI, IJCAI, AISTATS
                  </p>
                  <p>
                    <strong>Journal Reviewer:</strong> IEEE TPAMI, TKDE
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Footer -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <img src="https://hits.sh/yanwang.org.svg?label=Visitors&style=flat-square&color=4c71f2" alt="Visitor count" style="vertical-align:middle;">
                    <br>
                    Website template from <a href="https://jonbarron.info/">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>

