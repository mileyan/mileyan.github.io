<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yan Wang</title>
  <meta name="author" content="Yan Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <!-- Introduction Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Yan Wang
                  </p>
                  <p>
                    I am a Research Scientist at <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, specializing in autonomous driving, VLMs, VLA models, and reasoning. As a research tech lead, I develop Alpamayo-R1 VLA model with reasoning capabilities for autonomous vehicles.
                  </p>
                  <p>
                    I received my Ph.D. in Computer Science from <a href="https://www.cs.cornell.edu/">Cornell University</a>, where I was advised by <a href="https://www.cs.cornell.edu/~kilian/">Kilian Q. Weinberger</a> and <a href="https://www.cs.cornell.edu/~bharathh/">Bharath Hariharan</a>. Previously, I was a Research Scientist at <a href="https://waymo.com/">Waymo</a>. 
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:yanwang1993@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="https://github.com/mileyan">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/yanwang8">Linkedin</a> &nbsp;/&nbsp;
                    <a href="https://x.com/yan_wang_9">X</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=nZsD8XwAAAAJ">Google Scholar</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/yan_wang.JPG">
                    <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;" alt="profile photo" src="images/yan_wang.JPG">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm interested in Multimodal foundation models, 3D computer vision, autonomous driving. 
                    My research focuses on developing end-to-end learning systems for autonomous vehicles, 
                    and let the driving model can think itself.
                    I'm particularly interested in vision-language-action (VLA) models with reasoning capabilities 
                    and world models for embodied AI.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Selected Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- Alpamayo-R1 Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/alpamayo_r1.png" alt="Alpamayo-R1" style="width:160px;height:160px;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/Alpamayo-R1_1.pdf">
                    <span class="papertitle">Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail</span>
                  </a>
                  <br>
                  <br>
                  Project Lead
                  <br>
                  <br>
                  <em>NVIDIA Technical Report</em>, 2025
                  <br>
                  <a href="https://d1qx31qr3h6wln.cloudfront.net/publications/Alpamayo-R1_1.pdf">PDF</a>
                  <br>
                  <p></p>
                  <p>
                    A vision-language-action model integrating Chain of Causation reasoning with trajectory planning, demonstrating improvements on open-loop, closed-loop evaluation, and real-world road tests.
                  </p>
                </td>
              </tr>

              <!-- Paper 1 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cube_llm.png" alt="CubeLLM" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/pdf?id=yaQbTAD2JJ">
                    <span class="papertitle">Language-Image Models with 3D Understanding</span>
                  </a>
                  <br>
                  <br>
                  Jang Hyun Cho, Boris Ivanovic, Yulong Cao, Edward Schmerling, Yue Wang, Xinshuo Weng, Boyi Li, Yurong You, Philipp Kr√§henb√ºhl*, <strong>Yan Wang*</strong>, Marco Pavone*
                  <br>
                  <span style="font-size:12px;">* Co-advised</span>
                  <br>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://openreview.net/pdf?id=yaQbTAD2JJ">OpenReview</a>
                  <br>
                  <p></p>
                  <p>
                    Bridging vision-language models (VLMs) with 3D understanding.
                  </p>
                </td>
              </tr>

              <!-- STORM Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/storm.png" alt="STORM" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/pdf?id=M2NFWRPMUd">
                    <span class="papertitle">STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes</span>
                  </a>
                  <br>
                  <br>
                  Jiawei Yang*, Jiahui Huang*, Yuxiao Chen, <strong>Yan Wang</strong>, Boyi Li, Yurong You, Maximilian Igl, Apoorva Sharma, Peter Karkus, Danfei Xu, Boris Ivanovic, Yue Wang*, Marco Pavone*
                  <br>
                  <br>
                  <em>ICLR</em>, 2025
                  <br>
                  <a href="https://openreview.net/pdf?id=M2NFWRPMUd">OpenReview</a>
                  <br>
                  <p></p>
                  <p>
                    A feed-forward, self-supervised method for fast and accurate reconstruction of dynamic 3D scenes from sparse, multi-timestep, posed camera images.
                  </p>
                </td>
              </tr>

              <!-- Test-Time Scaling Paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/test_time.png" alt="Test-Time Scaling" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2503.24320">
                    <span class="papertitle">Can Test-Time Scaling Improve World Foundation Model?</span>
                  </a>
                  <br>
                  <br>
                  Wenyan Cong*, Hanqing Zhu*, Peihao Wang, Bangya Liu, Dejia Xu, Kevin Wang, David Z. Pan, <strong>Yan Wang</strong>, Zhiwen Fan, Zhangyang Wang
                  <br>
                  <br>
                  <em>COLM</em>, 2025
                  <br>
                  <a href="https://arxiv.org/pdf/2503.24320">arXiv</a>
                  <br>
                  <p></p>
                  <p>
                    Exploring test-time scaling strategies for world foundation models.
                  </p>
                </td>
              </tr>

              <!-- Paper 2 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/pseudo_lidar.png" alt="Pseudo-LiDAR" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/1812.07179">
                    <span class="papertitle">Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving</span>
                  </a>
                  <br>
                  <br>
                  <strong>Yan Wang</strong>, Wei-Lun Chao, Div Garg, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger
                  <br>
                  <br>
                  <em>CVPR</em>, 2019
                  <br>
                  <a href="https://arxiv.org/abs/1812.07179">arXiv</a>
                  <br>
                  <p></p>
                  <p>
                    Proposing the Pseudo-LiDAR representation that bridges the gap between image-based and LiDAR-based 3D object detection.
                  </p>
                </td>
              </tr>

              <!-- Paper 5 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/waymax.png" alt="Waymax" style="width:160px;height:auto;border-radius:4px;">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2310.08710">
                    <span class="papertitle">Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research</span>
                  </a>
                  <br>
                  <br>
                  Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein, Yiren Lu, Jean Harb, Xinlei Pan, <strong>Yan Wang</strong>, et al.
                  <br>
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2310.08710">arXiv</a>
                  /
                  <a href="https://github.com/waymo-research/waymax">code</a>
                  <br>
                  <p></p>
                  <p>
                    A high-performance, JAX-based simulator for autonomous driving research enabling large-scale RL training.
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          <!-- Service Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Academic Service</h2>
                  <p>
                    <strong>Workshop Organizer:</strong><br>
                    - ECCV 2024: <a href="https://mllmav.github.io/">Autonomous Vehicles meet Multimodal Foundation Models</a><br>
                    - CVPR 2023: <a href="https://cvpr2023.wad.vision">Autonomous Driving Workshop</a><br>
                    - CVPR 2022: <a href="https://cvpr2022.wad.vision">Autonomous Driving Workshop</a><br>
                    - CVPR 2021: <a href="https://www.adp3.org">Autonomous Driving Workshop</a>
                  </p>
                  <p>
                    <strong>Conference Reviewer:</strong> CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, ICRA, AAAI, IJCAI, AISTATS
                  </p>
                  <p>
                    <strong>Journal Reviewer:</strong> IEEE TPAMI, TKDE
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Footer -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Website template from <a href="https://jonbarron.info/">Jon Barron</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>

